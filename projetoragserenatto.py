# -*- coding: utf-8 -*-
"""ProjetoRAGSerenatto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocOv4EOUVFPH9KmDYnNbDWR4N87Z6AK4
"""



from llama_index.core import SimpleDirectoryReader
documentos = SimpleDirectoryReader(input_dir='./documentos') # Carregando documento pdf com as informações do café
documentos.input_files
docs = documentos.load_data()

docs[0].__dict__ # documento carregado com seus dados

#Fazendo o Chunking do arquivo
from llama_index.core.node_parser import SentenceSplitter
node_parser = SentenceSplitter(chunk_size=1200)
nodes = node_parser.get_nodes_from_documents(docs,show_progress=True)
len(nodes) # veirificando tamanho total dos nodes bate com o tamanho do documento

"""## Checando Nós

"""

nodes[0] # verificando os nodes

nodes[9]



from llama_index.embeddings.huggingface import HuggingFaceEmbedding
# Classe personalizada para adaptar a assinatura esperada pelo Chroma
class ChromaEmbeddingWrapper:
    def __init__(self, model_name): # Inicializa o modelo de embeddings do Hugging Face com o nome especificado
        self.model = HuggingFaceEmbedding(model_name=model_name)
        self.name = model_name # Add the name attribute

    def __call__(self, input): # Converte a entrada para um formato compatível com o HuggingFaceEmbedding
        return self.model.embed(input)

# Aqui está definido o modelo a ser usado para arquivos em português
embed_model_chroma = ChromaEmbeddingWrapper(model_name='intfloat/multilingual-e5-large')

"""#### Instalando dependencia para armazenar vetores no LlamaIndex"""



"""Criando Cliente persistente para armazenar e gerenciar os dados"""

import chromadb

db = chromadb.PersistentClient(path="./chroma_db")
chroma_client = db

collection_name  = 'documentos_serenatto'

try:
  # Remove the embedding_function argument here
  chroma_collection = chroma_client.get_or_create_collection(
      name= collection_name
  )
except Exception as e:
  print(f"Não foi possivel carregar ou criar a coleção {e}")

"""Aqui importamos as dependencias para armazenar o banco de dados vetorial no chromaDB e também para criar e carregar os indíces de armazenamento"""

from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext, VectorStoreIndex, load_index_from_storage

vector_store = ChromaVectorStore(chroma_collection=chroma_collection) #armazena os embeddings
storage_context = StorageContext.from_defaults(vector_store=vector_store)

embed_model = HuggingFaceEmbedding(model_name='intfloat/multilingual-e5-large')

index = VectorStoreIndex(nodes, storage_context=storage_context, embed_model=embed_model)

index = load_index_from_storage(storage_context, embed_model=embed_model)

from dotenv import load_dotenv
import os

load_dotenv()  # Carrega variáveis do .env
GROQ_API = os.getenv("GROQ_API")



from llama_index.llms.groq import Groq
llm = Groq(model='llama3-70b-8192', api_key=GROQ_API) # importamos o modelo de llm
query_engine = index.as_query_engine(llm=llm, similarity_top_k=2) # usando query engie para indexar com similaridade de 2 itens mais similares.

query_engine.query('Quais os grãos disponíveis') #Testando a query

query_engine.query('Quais os grãos disponíveis').response #resposta da query formatada

query_embedding = embed_model.get_text_embedding('Quais são os grãos disponíveis')# criando o embbeding vetorial para armazenar o texto em formato vetorial

#Consulta na coleção do ChromaDB para ver os dois resultados mais relevantes
chroma_collection.query(query_embedding, n_results=2, include=['distances','embeddings'])

chat_engine = index.as_chat_engine(mode='context', llm=llm) # criamos um chat engina para poder criar uma conversa no modo contexto

pergunta = chat_engine.chat('Quais grãos estão disponíveis?').response #testando o contexto
print(pergunta)

pergunta = chat_engine.chat('Me fale sobre o Bourbon vermelho').response #testando o contexto
print(pergunta)

pergunta = chat_engine.chat('Quanto ele custa e onde encontrá-lo?').response #testando o contexto
print(pergunta)

chat_engine.chat_history # analisando o histórico do chat

"""Agora usaremos o ChatSummaryMemoryBuffer para colocar um limite de tokens permitidos para não tornar o armazenamento do historico muito grande melhorando o gerenciamento"""

from llama_index.core.memory import ChatSummaryMemoryBuffer
memory = ChatSummaryMemoryBuffer(llm=llm, token_limit=256)

chat_engine = index.as_chat_engine( # criação do chat_engine no modo context para otimizar os resultados da busca do cliente
    chat_mode='context',
    llm=llm,
    memory=memory,
    system_prompt=('''Você é especialista em cafés especiais da Serenatto, uma loja online que vende grãos de cafés torrados.
       Sua função é tirar dúvidas de forma simpática e natural sobre os grãos disponíveis''')
)

chat_engine.reset() # resetar o historico do chat
response = chat_engine.chat('Olá') #começando uma nova conversa para avaliar as respostas
print(response)

response = chat_engine.chat('Me de detalhes sobre o café yirgacheffe')
print(response)

response = chat_engine.chat('Qual o preço dele?')
print(response)



#utlizando gradio para criação de uma interface gráfica de chatbot
import gradio as gr

# Criando a função para conversar com o chatbot
def converse_com_bot(message, chat_history):
    response = chat_engine.chat(message)

    if chat_history is None:
        chat_history = []

    chat_history.append({"role": "user", "content": message})
    chat_history.append({"role": "assistant", "content": response.response})

    return "", chat_history

# Criando a função para resetar o chatbot
def resetar_chat():
  chat_engine.reset()
  return []

with gr.Blocks() as demo:
  gr.Markdown('# Chatbot da Serenatto')

  chatbot = gr.Chatbot(type='messages')
  msg = gr.Textbox(label='Digite a sua mensagem')
  limpar = gr.Button('Limpar')

  msg.submit(converse_com_bot, [msg, chatbot], [msg, chatbot])
  limpar.click(resetar_chat, None, chatbot, queue=False)

demo.launch(debug=True)